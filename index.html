<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-108048997-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-108048997-1');
  </script>

  <title>Amir Bar</title>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&family=Open+Sans:wght@400;600&display=swap" rel="stylesheet">
  <style>
/* General Body Styling */
body {
  font-family: sans-serif;
  line-height: 1.6;
  margin: 0;
  padding: 0;
  color: #333;
}

a {
  color: #1772d0;
  text-decoration: none;
}

a:hover {
  color: #f09228;
}

/* Layout Container */
.container {
  max-width: 800px;
  margin: auto;
  padding: 20px;
}

/* Profile Section Styling */
.profile-section {
  display: flex;
  flex-wrap: wrap;
  justify-content: center;
  align-items: center;
  /* margin-bottom: 40px; */
}

.profile-text {
  flex: 1 1 60%;
  padding: 20px;
}

h1 {
  font-weight: normal;
  text-align: center;
}

.profile-text h1 {
  font-size: 2rem;
  /* margin-bottom: 10px; */
  font-weight: normal;
}

.profile-text p {
  margin: 10px 0;
}

.profile-image {
  flex: 1 1 30%;
  text-align: center;
}

.profile-image img {
  max-width: 200px;
  border-radius: 5%;
}

/* Social Links */
.social-links {
  text-align: center;
  margin: 20px 0;
}

.social-links a {
  margin: 0 10px;
}

/* Section Styling */
.section {
  margin-bottom: 40px;
}

.section-heading {
  font-size: 1.5rem;
  margin-bottom: 20px;
  /* font-weight: bold; */
}

/* List Styling */
ul {
  list-style: none;
  padding: 0;
}

/* Publications Section */
.publication {
  display: flex;
  flex-wrap: wrap;
  gap: 20px;
  border: 1px solid #ccc;
  border-radius: 8px;
  padding: 10px;
  overflow: hidden; /* Ensures child elements stay within the border */
}

.publication-media {
  flex: 1 1 35%; /* Allocates 35% of the space for media */
  position: relative;
  width: 100%;
  aspect-ratio: 4 / 1; /* Ensures a square layout */
  overflow: hidden;
  border-radius: 8px;
}

.publication-media img,
.publication-media video {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  object-fit: contain; /* Ensures the image or video fills the container */
  transition: opacity 0.3s ease-in-out;
}

.publication-media .one {
  opacity: 1; /* Show the "before" image by default */
  z-index: 1; /* Ensure it is on top */
}

.publication-media .two {
  opacity: 0; /* Hide the "after" image by default */
  z-index: 2; /* Stack it above the "before" image */
}

.publication-media:hover .one {
  opacity: 0; /* Hide the "before" image on hover */
}

.publication-media:hover .two {
  opacity: 1; /* Show the "after" image on hover */
}


.publication-details {
  flex: 1 1 60%;
}

.publication-details p {
  margin: 10px 0;
}

/* Responsive Design */
@media screen and (max-width: 768px) {
  .profile-section {
    flex-direction: column;
    align-items: center;
  }

  /* .profile-text {
    text-align: center;
  } */

  .publication {
    flex-direction: column;
  }

  .publication-media {
    flex: 1 1 100%;
  }

  .publication-details {
    flex: 1 1 100%;
  }
}

.news-section {
  margin-bottom: 40px;
}

.news-list {
  list-style-type: disc;
  margin: 0;
  padding-left: 20px;
}

.news-list li {
  /* margin-bottom: 10px; */
  line-height: 1.6;
}

.news-list a {
  color: #1772d0;
  text-decoration: none;
}

.news-list a:hover {
  color: #f09228;
}
.researchers-section {
  margin-bottom: 40px;
}

.researchers-list {
  list-style-type: none;
  padding: 0;
  margin: 0;
}

.researchers-list li {
  margin-bottom: 10px;
}

.researchers-list a {
  color: #1772d0;
  text-decoration: none;
  font-weight: bold;
}

.researchers-list a:hover {
  color: #f09228;
}
.researchers-section {
  margin-bottom: 40px;
  padding: 20px;
  background-color: #f9f9f9;
  border: 1px solid #ccc;
  border-radius: 8px;
}
/* Patents Section */
.patents-section {
  margin-bottom: 40px;
}

.section-heading {
  font-size: 1.5rem;
  margin-bottom: 20px;
  font-weight: bold;
}

.patents-list {
  list-style-type: disc;
  padding-left: 20px;
  margin: 0;
}

.patents-list li {
  margin-bottom: 10px;
  font-size: 1rem;
  line-height: 1.5;
}
.publication {
  transition: transform 0.3s ease, box-shadow 0.3s ease;
}

.publication:hover {
  transform: scale(1.02); /* Slight zoom on hover */
  box-shadow: 0 0 15px; /* Intensify glow on hover */
}
.coming-soon .coming-soon-badge {
  display: inline-block;
  background-color: #f09228;
  color: white;
  padding: 2px 8px;
  font-size: 0.9rem;
  font-weight: bold;
  border-radius: 12px;
  margin-top: 5px;
}
.coming-soon img {
  opacity: 0.5; /* Optional: Make the placeholder image look subdued */
}


  </style>
</head>

<body>
  <div class="container">
    <!-- Profile Section -->
    <h1>Amir Bar</h1>

    <div class="profile-section">
      <div class="profile-image">
        <img src="images/DSC06400_2.png" alt="Amir Bar">
      </div>

      <div class="profile-text">
        <!-- <h2>Shalom!</h2> -->
        <p>
          I am a Postdoctoral Researcher at Meta AI (FAIR), working with <a href="http://yann.lecun.com/">Yann LeCun</a>.
          Prior to this, I was a <i>Ph.D. candidate</i> at Tel Aviv University and <a href="https://bair.berkeley.edu/">UC Berkeley</a>, advised by <a href="https://cs3801.wixsite.com/amirgloberson">Amir Globerson</a> and <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>. 
          <!-- Before my PhD, I've pioneered AI for medical imaging as an <i>AI Research Lead</i> at <a href="https://www.zebra-med.com/">Zebra Medical Vision</a> (acquired). My research team developed multiple FDA cleared algorithms for automatic analysis of medical images (e.g, [<a href="https://www.businesswire.com/news/home/20200518005487/en/Zebra-Medical-Vision-Secures-5th-FDA-Clearance">1</a>, <a href="https://www.radiologybusiness.com/topics/artificial-intelligence/ai-triage-brain-bleeds-gains-fda-clearance">2</a>]). -->
        </p>
        <p>My goal is to teach computers to <b>perceive, reason, and act</b> in the world from visual data, using little to no supervision.</p>
      </div>
    </div>

    <!-- Social Links -->
    <div class="social-links">
      <a href="mailto:amirb4r@gmail.com">Email</a>
      <a href="https://twitter.com/_amirbar">Twitter</a>
      <a href="https://github.com/amirbar">GitHub</a>
      <a href="https://scholar.google.co.il/citations?user=L__n1LUAAAAJ&hl=en">Google Scholar</a>
      <a href="https://www.linkedin.com/in/amirbar/">LinkedIn</a>
      <a href="Amir_Bar_CV.pdf">CV</a>
    </div>

    <div class="section news-section">
        <h2 class="section-heading">News</h2>
        <ul class="news-list">
            <!-- <li>
            Blavatnik Prize for Computer Science Doctoral Fellows, 2025.
           </li> -->

           
           <li>
            Our paper <a href="nwm"><i>Navigation World Models</i></a> won the Best Paper Honorable Mention at CVPR 2025.
           </li>
           <li>
            I was a guest on the <a href="https://x.com/RoboPapers/status/1936424617922728270/" target="_blank">RoboPapers</a> podcast where I talked about Navigation World Models.
          </li>
           <li>
            I was a guest on the <a href="https://twimlai.com/podcast/twimlai/decoding-animal-behavior-to-train-robots-with-egopet/" target="_blank">TWIML AI podcast</a> where I talked about my recent research.
          </li>
          <li>
            I organized the first workshop on <a href="https://prompting-in-vision.github.io/index_cvpr24.html" target="_blank">Prompting in Vision</a> at CVPR 24'.
          </li>
          <li>I served as an Assistant Program Chair at <a href="https://nips.cc/Conferences/2023/Committees" target="_blank">NeurIPS 2023</a>.</li>
        </ul>
      </div>
      

    <!-- Publications Section -->
    <section class="section">
      <div class="section-heading">Selected Publications</div>

      <div class="publication">
        <div class="publication-media">
            <img src="images/WebSSL.png"/>
        </div>
        <div class="publication-details">
          <p>
            <a href="https://arxiv.org/abs/2504.01017"><strong>Scaling Language-Free Visual Representation Learning</strong></a><br>
            David Fan*, Shengbang Tong*, Jiachen Zhu, Koustuv Sinha, Zhuang Liu, Xinlei Chen, Michael Rabbat, Nicolas Ballas, Yann LeCun, <strong>Amir Bar</strong><sup>&dagger;</sup>, Saining Xie<sup>&dagger;</sup> <br>
            <em>ICCV</em>, 2025 <span style="color:red">[<i>Highlight</i>]</span><br>
            <a href="https://davidfan.io/webssl/">Project Page</a> | <a href="https://github.com/facebookresearch/webssl/">Code</a> | <a href="https://huggingface.co/collections/facebook/web-ssl-68094132c15fbd7808d1e9bb">Model Weights</a>
          </p>
          <p>Vision-only models scale with model and data size, catching up with CLIP models without language supervision.</p>
        </div>
      </div>

      <div class="publication">
        <div class="publication-media">
            <img src="images/fpolygons.png"/>
        </div>
        <div class="publication-details">
          <p>
            <a href="https://arxiv.org/abs/2502.15969"><strong>Forgotten Polygons: Multimodal Large Language Models are Shape-Blind
</strong></a><br>
William Rudman*, Michal Golovanevsky*, <strong>Amir Bar</strong>, Vedant Palit, Yann LeCun, Carsten Eickhoff, Ritambhara Singh
<br>
            <em>ACL</em>, 2025<br>
            <a href="https://github.com/rsinghlab/Shape-Blind/">Code</a>
          </p>
          <p>MLLMs struggle with understanding geometic primitives.</p>
        </div>
      </div>

      <div class="publication">
        <div class="publication-media">
            <img src="images/before_cmtask.png" class="one" alt="Before Image" />
            <img src="images/after_cmtask.png" class="two" alt="After Image" />
        </div>
        <div class="publication-details">
          <p>
            <a href="https://arxiv.org/abs/2410.22330"><strong>Vision-Language Models Create Cross-Modal Task Representations</strong></a><br>
            Grace Luo, Trevor Darrell, <strong>Amir Bar</strong><br>
            <em>ICML</em>, 2025<br>
            <a href="https://vlm-cross-modal-reps.github.io/">Project Page</a> | <a href="https://github.com/g-luo/vlm_cross_modal_reps/">Code</a>
          </p>
          <p>Vision-Language Models (VLMs) answer questions by mapping inputs to shared cross-modal task representations, an important inductive bias that helps explain VLM's success.</p>
        </div>
      </div>
      <!-- Example Publication -->
      <div class="publication highlighted">
        <div class="publication-media">
            <img src="images/before_nwm.png" class="one" alt="Before Image" />
            <video autoplay muted loop id="nwm" class="two">
                <source src="images/after_nwm.mp4">
            </video>          
        </div>
        <div class="publication-details">
          <p>
            <a href="https://arxiv.org/abs/2412.03572"><strong>Navigation World Models</strong></a><br>
            <strong>Amir Bar</strong>, Gaoyue Zhou, Danny Tran, Trevor Darrell, Yann LeCun<br> 
            <em>CVPR</em>, 2025 <span style="color:red"> [<i>Best Paper Honorable Mention</i>]</span><br>
            <a href="https://www.amirbar.net/nwm/">Project Page</a> | <a href="https://github.com/facebookresearch/nwm/">Code</a> | <a href="https://huggingface.co/facebook/nwm/">Models</a>
          </p>
          <p>Planning trajectories by simulating them using a video world model. Our conditional diffusion transformer (CDiT) generates videos that enable planning, counterfactual reasoning, and action.</p>
        </div>
      </div>


      
      <!-- Publication: EgoPet -->
      <div class="publication">
        <div class="publication-media">
            <video muted loop preload="metadata">
              <source src="images/egopet.mov" type="video/mp4" />
              Your browser does not support the video tag.
            </video>
          </div>
        <div class="publication-details">
          <p>
            <a href="https://arxiv.org/abs/2404.09991"><strong>EgoPet: Egomotion and Interaction Data from an Animal's Perspective</strong></a><br>
            <strong>Amir Bar</strong>, Arya Bakhtiar, Danny Tran, Antonio Loquercio, Jathushan Rajasegaran, Yann LeCun, Amir Globerson, Trevor Darrell<br>
            <em>ECCV</em>, 2024<br>
            <a href="https://www.amirbar.net/egopet/">Project Page</a> | <a href="https://github.com/bakhtiararya/EgoPet_dataset_download/">Data</a> | <a href="https://github.com/dannyTran123/egopet/">Code</a>
          </p>
          <p>We present EgoPet, a new egocentric video dataset of animals.</p>
        </div>
      </div>
      
      <!-- Publication: Finding Visual Task Vectors -->
      <div class="publication">
        <div class="publication-media">
            <img src="images/vp2.png" class="one" alt="Before Image" />
            <img src="images/vp.png" class="two" alt="After Image" />
          </div>
        <div class="publication-details">
          <p>
            <a href="https://arxiv.org/abs/2404.05729"><strong>Finding Visual Task Vectors</strong></a><br>
            Alberto Hojel, Yutong Bai, Trevor Darrell, Amir Globerson, <strong>Amir Bar</strong><br>
            <em>ECCV</em>, 2024<br>
            <a href="https://github.com/alhojel/visual_task_vectors">Code</a>
          </p>
          <p>We discover Task Vectors, latent representations in a model's activation space that encode task information. We propose a method to find and use them to guide models for performing tasks.</p>
        </div>
      </div>
      
      <!-- Publication: Stochastic Positional Embeddings -->
      <div class="publication">
        <div class="publication-media">
          <img src="images/stop_before.png" class="one" alt="Before Image" />
          <img src="images/stop_after.png" class="two" alt="After Image" />
        </div>
        <div class="publication-details">
          <p>
            <a href="https://arxiv.org/abs/2308.00566"><strong>Stochastic Positional Embeddings Improve Masked Image Modeling</strong></a><br>
            <strong>Amir Bar</strong>, Florian Bordes, Assaf Shocher, Mahmoud Assran, Pascal Vincent, Nicolas Ballas, Trevor Darrell, Amir Globerson, Yann LeCun<br>
            <em>ICML</em>, 2024<br>
            <a href="https://github.com/amirbar/StoP">Code</a>
          </p>
          <p>Modeling location uncertainties via stochastic positional embeddings (StoP) improves masked image modeling.</p>
        </div>
      </div>
            
      <!-- Publication: IMProv -->
      <div class="publication">
        <div class="publication-media">
            <img src="images/improv_before.png" class="one" alt="Before Image" />
            <img src="images/improv_after.png" class="two" alt="After Image" />
        </div>
        <div class="publication-details">
          <p>
            <a href="https://arxiv.org/abs/2312.01771"><strong>IMProv: Inpainting-based Multimodal Prompting for Computer Vision Tasks</strong></a><br>
            Jiarui Xu, Yossi Gandelsman, <strong>Amir Bar</strong>, Jianwei Yang, Jianfeng Gao, Trevor Darrell, Xiaolong Wang<br>
            <em>TMLR</em>, 2024<br>
            <a href="https://jerryxu.net/IMProv">Project Page</a> | <a href="https://github.com/xvjiarui/IMProv">Code/Data</a> | <a href="https://colab.research.google.com/drive/1mJlZutYhVcwW9AMFcuEZ-d2SvuLnXLIh?usp=sharing">Demo</a>
          </p>
          <p>IMProv performs multimodal in-context learning to solve computer vision tasks.</p>
        </div>
      </div>

      <!-- Publication: Sequential Modeling Enables Scalable Learning -->
<div class="publication">
    <div class="publication-media">
      <img src="images/lvm_after.png" alt="LVM">
    </div>
    <div class="publication-details">
      <p>
        <a href="https://arxiv.org/abs/2312.00785"><strong>Sequential Modeling Enables Scalable Learning for Large Vision Models</strong></a><br>
        Yutong Bai, Xinyang Geng, Karttikeya Mangalam, <strong>Amir Bar</strong>, Alan Yuille, Trevor Darrell, Jitendra Malik, Alexei A Efros<br>
        <em>CVPR</em>, 2024<br>
        <a href="https://yutongbai.com/lvm.html">Project Page</a> | <a href="https://github.com/ytongbai/LVM">Code</a>
      </p>
      <p>Large Vision Model trained on 420B tokens; exhibits interesting in-context learning capabilities.</p>
    </div>
  </div>
  
  <!-- Publication: Visual Prompting via Image Inpainting -->
  <div class="publication">
    <div class="publication-media">
        <img src="images/vp_before.png" class="one" alt="Before Image" />
        <img src="images/vp_after.png" class="two" alt="After Image" />
    </div>
    <div class="publication-details">
      <p>
        <a href="https://arxiv.org/abs/2209.00647"><strong>Visual Prompting via Image Inpainting</strong></a><br>
        <strong>Amir Bar</strong>*, Yossi Gandelsman*, Trevor Darrell, Amir Globerson, Alexei A. Efros<br>
        <em>NeurIPS</em>, 2022<br>
        <a href="https://yossigandelsman.github.io/visual_prompt/">Project Page</a> | <a href="https://github.com/amirbar/visual_prompting">Code/Data</a>
      </p>
      <p>Adapt a pre-trained visual model to novel downstream tasks without task-specific fine-tuning or any model modification.</p>
    </div>
  </div>
  
  <!-- Publication: Bringing Image Scene Structure to Video -->
  <div class="publication">
    <div class="publication-media">
        <img src="images/svit.png" alt="SViT Image" />
    </div>
          <div class="publication-details">
      <p>
        <a href="https://arxiv.org/abs/2206.06346"><strong>Bringing Image Scene Structure to Video via Frame-Clip Consistency of Object Tokens</strong></a><br>
        Elad Ben-Avraham, Roei Herzig, Karttikeya Mangalam, <strong>Amir Bar</strong>, Anna Rohrbach, Leonid Karlinsky, Trevor Darrell, Amir Globerson<br>
        <em>NeurIPS</em>, 2022<br>
        <a href="https://eladb3.github.io/SViT/">Project Page</a> | <a href="https://github.com/eladb3/SViT">Code</a>
      </p>
      <p>Incorporating image-level scene structure during training improves video transformers.</p>
    </div>
  </div>
  
  <!-- Publication: Object-Region Video Transformers -->
  <div class="publication">
    <div class="publication-media">
        <img src="images/ORViT.gif" alt="ORViT Animation" />
      </div>
    <div class="publication-details">
      <p>
        <a href="https://arxiv.org/abs/2110.06915"><strong>Object-Region Video Transformers</strong></a><br>
        Roei Herzig, Elad Ben-Avraham, Karttikeya Mangalam, <strong>Amir Bar</strong>, Gal Chechik, Anna Rohrbach, Trevor Darrell, Amir Globerson<br>
        <em>CVPR</em>, 2022<br>
        <a href="https://roeiherz.github.io/ORViT/">Project Page</a> | <a href="https://github.com/roeiherz/ORViT">Code</a>
      </p>
      <p>Incorporating objects into transformer layers improves video transformers.</p>
    </div>
  </div>
  
  <!-- Publication: DETReg -->
  <div class="publication">
    <div class="publication-media">
        <img src="images/detreg_before.png" class="one" alt="Before Image" />
        <img src="images/detreg_after.png" class="two" alt="After Image" />
      </div>
          <div class="publication-details">
      <p>
        <a href="https://arxiv.org/abs/2106.04550"><strong>DETReg: Unsupervised Pretraining with Region Priors for Object Detection</strong></a><br>
        <strong>Amir Bar</strong>, Xin Wang, Vadim Kantorov, Colorado J Reed, Roei Herzig, Gal Chechik, Anna Rohrbach, Trevor Darrell, Amir Globerson<br>
        <em>CVPR</em>, 2022<br>
        <a href="https://www.amirbar.net/detreg/">Project Page</a> | <a href="https://github.com/amirbar/detreg">Code</a> | <a href="https://youtu.be/1rxz_SWB7gQ?t=443">Video</a> | <a href="https://colab.research.google.com/drive/1ByFXJClyzNVelS7YdT53_bMbwYeMoeNb?usp=sharing">Demo</a>
      </p>
      <p>Pretraining transformers to localize potential objects improves object detection.</p>
    </div>
  </div>
  
  <!-- Publication: Compositional Video Synthesis with Action Graphs -->
  <div class="publication">
    <div class="publication-media">
        <img src="images/ag2vid_task.gif" alt="Action Graphs Animation" />
      </div>
    <div class="publication-details">
      <p>
        <a href="https://arxiv.org/abs/2006.15327"><strong>Compositional Video Synthesis with Action Graphs</strong></a><br>
        <strong>Amir Bar</strong>*, Roei Herzig*, Xiaolong Wang, Anna Rohrbach, Gal Chechik, Trevor Darrell, Amir Globerson<br>
        <em>ICML</em>, 2021<br>
        <a href="https://roeiherz.github.io/AG2Video/">Project Page</a> | <a href="https://github.com/roeiherz/AG2Video">Code</a> | <a href="https://www.youtube.com/watch?v=9HOFC4ffOuY&feature=youtu.be&t=8660">Video</a>
      </p>
      <p>We introduce Action Graphs, a structure that can better capture the compositional and hierarchical nature of actions.</p>
    </div>
  </div>
  
  <!-- Publication: Learning Canonical Representations for Scene Graphs -->
  <div class="publication">
    <div class="publication-media">
        <img src="images/canonical_baseline.png" class="one" alt="Before Image" />
        <img src="images/canonical_ours.png" class="two" alt="After Image" />
      </div>
          <div class="publication-details">
      <p>
        <a href="https://arxiv.org/abs/1912.07414"><strong>Learning Canonical Representations for Scene Graph to Image Generation</strong></a><br>
        Roei Herzig*, <strong>Amir Bar</strong>*, Huijuan Xu, Gal Chechik, Trevor Darrell, Amir Globerson<br>
        <em>ECCV</em>, 2020<br>
        <a href="https://roeiherz.github.io/CanonicalSg2Im/">Project Page</a> | <a href="https://github.com/roeiherz/CanonicalSg2Im">Code</a> | <a href="https://youtu.be/9HOFC4ffOuY?t=8543">Video</a>
      </p>
      <p>We present a model for Scene Graph to Image generation which is more robust to complex input scene graphs.</p>
    </div>
  </div>
  
  <!-- Publication: Learning Individual Styles of Conversational Gesture -->
  <div class="publication">
    <div class="publication-media">
        <img src="images/gestures_CVPR_2019_demo.gif" alt="Gestures Animation" />
    </div>
          <div class="publication-details">
      <p>
        <a href="https://arxiv.org/abs/1906.04160"><strong>Learning Individual Styles of Conversational Gesture</strong></a><br>
        Shiry Ginosar*, <strong>Amir Bar</strong>*, Gefen Kohavi, Caroline Chan, Andrew Owens, Jitendra Malik<br>
        <em>CVPR</em>, 2019<br>
        <a href="https://people.eecs.berkeley.edu/~shiry/projects/speech2gesture/index.html">Project Page</a> | <a href="https://github.com/amirbar/speech2gesture">Code</a> | <a href="https://drive.google.com/drive/folders/1qvvnfGwas8DUBrwD4DoBnvj8anjSLldZ?usp=sharing">Data</a>
      </p>
      <p>We predict plausible gestures to go along with someone's speech.</p>
    </div>
  </div>
  
  <!-- Publication: Language Generation with RNNs -->
  <div class="publication">
    <div class="publication-media">
        <code>
          <h4>Sample:</h4>
          The man allowed that about health captain played that alleged to Marks live up in the club comes the handed up moved to a brief
        </code>
      </div>
          <div class="publication-details">
      <p>
        <a href="https://arxiv.org/abs/1706.01399"><strong>Language Generation with Recurrent Generative Adversarial Networks without Pre-training</strong></a><br>
        Ofir Press*, <strong>Amir Bar</strong>*, Ben Bogin*, Jonathan Berant, Lior Wolf<br>
        <em>1st Workshop on Learning to Generate Natural Language at ICML</em>, 2017<br>
        <a href="https://github.com/amirbar/rnn.wgan">Code</a>
      </p>
      <p>We show that recurrent neural networks can be trained to generate text with GANs from scratch and vastly improve the quality of generated sequences compared to a convolutional baseline.</p>
    </div>
  </div>
  
    
    </section>
  
    <div class="section medical-section">
        <h2 class="section-heading">Medical Papers</h2>
      
        <!-- Medical Paper 1 -->
        <div class="publication">
          <div class="publication-media">
            <img src="images/cf_before.png" class="one" alt="Before Image">
            <img src="images/cf_after.png" class="two" alt="After Image">
          </div>
          <div class="publication-details">
            <p>
              <a href="https://arxiv.org/abs/2010.03739"><strong>3D Convolutional Sequence to Sequence Model for Vertebral Compression Fractures Identification in CT</strong></a><br>
              David Chettrit, Tomer Meir, Hila Lebel, Mila Orlovsky, Ronen Gordon, Ayelet Akselrod-Ballin, <strong>Amir Bar</strong><br>
              <em>MICCAI</em>, 2020<br>
              Press: <a href="https://nocamels.com/2020/05/fda-zebra-medical-compression-fractures/">1</a>, <a href="https://www.businesswire.com/news/home/20200518005487/en/Zebra-Medical-Vision-Secures-5th-FDA-Clearance">2</a>
            </p>
            <p>We present a novel architecture used to detect vertebral compression fractures in Chest and Abdomen CT.</p>
          </div>
        </div>
      
        <!-- Medical Paper 2 -->
        <div class="publication">
          <div class="publication-media">
            <img src="images/nature.png" alt="Nature Paper Image">
          </div>
          <div class="publication-details">
            <p>
              <a href="https://www.nature.com/articles/s41591-019-0720-z"><strong>Automated Opportunistic Osteoporotic Fracture Risk Assessment Using Computed Tomography Scans to Aid in FRAX Underutilization</strong></a><br>
              Noa Dagan, Eldad Elnekave, Noam Barda, Orna Bregman-Amitai, <strong>Amir Bar</strong>, Mila Orlovsky, Eitan Bachmat, Ran D. Balicer<br>
              <em>Nature Medicine</em>, 2020<br>
              Press: <a href="https://www.healthdatamanagement.com/news/algorithm-assess-ct-scans-to-find-patients-at-risk-for-bone-fractures">1</a>, <a href="https://www.prnewswire.com/il/news-releases/new-study-published-on-nature-medicine-unveils-the-power-of-ai-in-predicting-osteoporotic-fractures-300990287.html">2</a>
            </p>
            <p>We demonstrate it is feasible to automatically evaluate risk based on routine abdomen or chest computed tomography (CT) scans.</p>
          </div>
        </div>
      
        <div class="publication">
            <div class="publication-media">
              <img src="images/ich.png" alt="ICH Classification">
            </div>
            <div class="publication-details">
              <p>
                <a href="https://arxiv.org/abs/1907.00148"><strong>Improved ICH Classification Using Task-Dependent Learning</strong></a><br>
                <strong>Amir Bar</strong>, Michal Mauda-Havakuk, Yoni Turner, Michal Safadi, Eldad Elnekave<br>
                <em>ISBI</em>, 2019<br>
                <a href="https://sg.finance.yahoo.com/news/zebra-medical-vision-announces-ce-140400903.html">Press</a>
              </p>
              <p>
                Intracranial hemorrhage (ICH) is among the most critical and time-sensitive findings to be detected on Head CT. We present a new architecture designed for optimal triaging of Head CTs, with the goal of decreasing the time from CT acquisition to accurate ICH detection.
              </p>
            </div>
          </div>
        
          <!-- Medical Paper: Dual-Energy X-Ray Absorptiometry -->
          <div class="publication">
            <div class="publication-media">
              <img src="images/before_dexa.png" class="one" alt="Before Image">
              <img src="images/after_dexa.png" class="two" alt="After Image">
            </div>
            <div class="publication-details">
              <p>
                <a href="https://arxiv.org/pdf/1706.01671"><strong>Simulating Dual-Energy X-Ray Absorptiometry in CT Using Deep-Learning Segmentation Cascade</strong></a><br>
                Arun Krishnaraj, Spencer Barrett, Orna Bregman-Amitai, Michael Cohen-Sfady, <strong>Amir Bar</strong>, David Chettrit, Mila Orlovsky, Eldad Elnekave<br>
                <em>Journal of the American College of Radiology</em>, 2019
              </p>
              <p>
                Osteoporosis is an underdiagnosed condition despite effective screening modalities. This study describes a method to simulate lumbar DEXA scores from routine CT studies using a machine-learning algorithm.
              </p>
            </div>
          </div>
        
          <!-- Medical Paper: PHT-bot -->
          <div class="publication">
            <div class="publication-media">
              <img src="images/pht_before.png" class="one" alt="Before Image">
              <img src="images/pht_after.png" class="two" alt="After Image">
            </div>
            <div class="publication-details">
              <p>
                <a href="https://arxiv.org/pdf/1905.11773"><strong>PHT-bot: A Deep Learning-Based System for Automatic Risk Stratification of COPD Patients Based Upon Signs of Pulmonary Hypertension</strong></a><br>
                David Chettrit, Orna Bregman Amitai, Itamar Tamir, <strong>Amir Bar</strong>, and Eldad Elnekave<br>
                <em>SPIE</em>, 2019
              </p>
              <p>
                Chronic Obstructive Pulmonary Disease (COPD) is a leading cause of morbidity and mortality worldwide. We applied a deep learning algorithm to detect those at risk.
              </p>
            </div>
          </div>
        
          <!-- Medical Paper: Compression Fractures Detection -->
          <div class="publication">
            <div class="publication-media">
              <img src="images/cf_before.png" class="one" alt="Before Image">
              <img src="images/cf_after.png" class="two" alt="After Image">
            </div>
            <div class="publication-details">
              <p>
                <a href="https://arxiv.org/abs/1706.01671"><strong>Compression Fractures Detection on CT</strong></a><br>
                <strong>Amir Bar</strong>, Lior Wolf, Orna Bregman Amitai, Eyal Toledano, Eldad Elnekave<br>
                <em>SPIE</em>, 2017<br>
                Press: <a href="https://www.businesswire.com/news/home/20170117005827/en/Zebra-Medical-Vision-Announces-New-Algorithm-Detect">1</a>, <a href="https://www.calcalist.co.il/internet/articles/0,7340,L-3705948,00.html">2</a>
              </p>
              <p>
                The presence of a vertebral compression fracture is highly indicative of osteoporosis and represents the single most robust predictor for development of a second osteoporotic fracture. We present an automated method for detecting spine compression fractures in CT scans.
              </p>
            </div>
          </div>
            </div>
    <div class="section patents-section">
        <h2 class="section-heading">Patents</h2>
        <ul class="patents-list">
            <li>Systems and methods for automated detection of visual objects in medical images. US Patent 11,776,243</li>
            <li>Cross modality training of machine learning models. US Patent 11,587,228</li>
            <li>Identification of a contrast phase depicted in a medical image. US Patent 11,727,087</li>
        </ul>
    </div>
              
    <div class="section researchers-section">
        <h2 class="section-heading">Undergraduate and MA Collaborators</h2>
        <p>If you are a student interested in collaborating on research projects or looking for advice, please reach out.</p>
        <ul class="researchers-list">
          <li>
            <a href="https://dannytran123.github.io/" target="_blank">Danny Tran</a>, now incoming PhD at UT Austin.
          </li>
          <li>
            <a href="https://alhojel.github.io/" target="_blank">Alberto Hojel</a>, now founder and CEO at Lucid Simulations.
          </li>
          <li>
            <a href="https://scholar.google.com/citations?user=Y83XwowAAAAJ&hl=en" target="_blank">Arya Bakhtiar</a>, now MA at Stanford
          </li>
          <li>
            <a href="https://scholar.google.com/citations?hl=en&user=AzlUrvUAAAAJ" target="_blank">Dantong Niu</a>, now PhD at UC Berkeley
          </li>
          <li>
            <a href="https://scholar.google.com/citations?hl=en&user=G8sxJ_8AAAAJ" target="_blank">Elad Ben-Avraham</a>, now at Amazon
          </li>
        </ul>
      </div>
</div>

  
  <script>
    document.querySelectorAll('.publication-media video').forEach(video => {
    video.addEventListener('mouseenter', () => {
      video.play();
    });

    video.addEventListener('mouseleave', () => {
      video.pause();
      video.currentTime = 0; // Reset to the first frame
    });
  });

    
  </script>
</body>

</html>
